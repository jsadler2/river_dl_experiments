import os
import xarray as xr
#from combine_experiment_metrics import combine_metrics
import pandas as pd

# add scripts dir to path
code_dir = "/home/jsadler/drb-dl-model/river_dl"
shell.prefix("module load analytics cuda100/toolkit/10.0.130 \n \
              run_training -e /home/jsadler/.conda/envs/rgcn --no-node-list ")

from river_dl.preproc_utils import prep_data
from river_dl.postproc_utils import predict_from_file, overall_metrics, combined_reach_specific, combine_csvs
data_dir = "/home/jsadler/drb-dl-model/data/in"
out_dir = "/caldera/projects/usgs/water/iidd/datasci/drb_ml_model/experiments/subset/wgt_experiment"
x_vars = ['seg_rain', 'seg_tave_air', 'seginc_swrad', 'seg_length', 'seginc_potet', 'seg_slope', 'seg_humid', 'seg_elev']
num_replicates = 5


rule all:
    input:
        f"{out_dir}/exp_combined_metrics.nc",
        f"{out_dir}/exp_combined_reach_metrics.nc"


rule prep_io_data:
    input:
         f"{data_dir}/obs_temp_full",
         f"{data_dir}/obs_flow_full",
         f"{data_dir}/uncal_sntemp_input_output_subset",
         f"{data_dir}/distance_matrix_subset.npz",
         catch_attr = f"{data_dir}/seg_attr_drb.feather"
    output:
        "{outdir}/prepped.npz"
    params:
        test_start_date='2004-09-30',
        n_test_yr=12
    run:
        prep_data(input[0], input[1], input[2], input[3], x_vars,
                  catch_prop_file=input['catch_attr'],
                  reduce_temp_trn=config.get('reduce_temp_trn', 0.95),
                  reduce_flow_trn=config.get('reduce_flow_trn', 0),
                  exclude_file=config.get('exclude_file'),
                  test_start_date=params.test_start_date,
                  log_q=True,
                  n_test_yr=params.n_test_yr, out_file=output[0])


# this must be a shell command so that the correct GPU libraries are loaded and used
rule train:
    input:
         "{outdir}/ex_{experiment}/prepped.npz"
    output:
        directory("{outdir}/ex_{experiment}/{run_id}/trained_weights/"),
        directory("{outdir}/ex_{experiment}/{run_id}/pretrained_weights/")
    params:
        # getting the base path to put the training outputs in
        # I omit the last slash (hence '[:-1]' so the split works properly
        run_dir=lambda wildcards, output: os.path.split(output[0][:-1])[0],
        flow_in_temp="-q", 
        pt_epochs=200,
        ft_epochs=100,
        pt_tmp_wgt=lambda wildcards: config['experiments'][wildcards.experiment]['pt_tmp_wgt'],
        ft_tmp_wgt=lambda wildcards: config['experiments'][wildcards.experiment]['ft_tmp_wgt']
    shell:
         """
         "python {code_dir}/train_model.py -o {params.run_dir} -i {input[0]} -m {output[0]} -p {params.pt_epochs} -f {params.ft_epochs} {params.flow_in_temp} --pt_temp_wgt {params.pt_tmp_wgt} --ft_temp_wgt {params.ft_tmp_wgt}" 
         """

rule make_predictions:
    input: 
        "{outdir}/ex_{experiment}/{run_id}/trained_weights/",
        "{outdir}/ex_{experiment}/prepped.npz",
    params:
        hidden_size=20,
        half_tst=True,
    output:
        "{outdir}/ex_{experiment}/{run_id}/{partition}_preds.feather",
    group: "post_proc"
    run:
        predict_from_file(input[0], input[1], params.hidden_size,
                          wildcards.partition, output[0], flow_in_temp=True,
                          half_tst=params.half_tst, logged_q=True)


rule calc_overall_metrics:
    input:
         "{outdir}/ex_{experiment}/{run_id}/{partition}_preds.feather",
         expand("{data_dir}/obs_{variable}_full", data_dir=data_dir, allow_missing=True)
    output:
         "{outdir}/ex_{experiment}/{run_id}/{partition}_{variable}_metrics.csv"
    group: "post_proc"
    run:
         overall_metrics(input[0], input[1], wildcards.variable, wildcards.partition, output[0])


rule combined_metrics:
    input:
        expand("{outdir}/ex_{experiment}/{run_id}/{partition}_{variable}_metrics.csv", partition=['trn', 'tst'],
               variable=['temp', 'flow'], allow_missing=True)
    output:
         "{outdir}/ex_{experiment}/{run_id}/combined_metrics.csv"
    group: "post_proc"
    run:
        combine_csvs(input, output[0])


def combine_replicate_csvs(csvs, exp_id, seg_id_idx=False):
    df_list = []
    for i, metric_file in enumerate(csvs):
        df = pd.read_csv(metric_file)
        df['run_id'] = i
        df_list.append(df)
    df = pd.concat(df_list, axis=0)
    df['exp_id'] = exp_id
    idx_cols = ['run_id', 'variable', 'partition', 'exp_id']
    if seg_id_idx:
        idx_cols.append('seg_id_nat')
    ds = df.set_index(idx_cols).to_xarray()
    return ds

rule combined_run_metrics:
    input:
         expand("{outdir}/ex_{experiment}/{run_id}/combined_metrics.csv",
                run_id=list(range(num_replicates)), allow_missing=True)
    output:
         "{outdir}/ex_{experiment}/overall_metrics.nc"
    group: "post_proc"
    run:
        ds = combine_replicate_csvs(input, wildcards.experiment)
        ds.to_netcdf(output[0])


rule calc_reach_specific_metrics:
    input: 
         "{outdir}/ex_{experiment}/{run_id}/trn_preds.feather",
         "{outdir}/ex_{experiment}/{run_id}/tst_preds.feather",
         f"{data_dir}/obs_temp_full",
         f"{data_dir}/obs_flow_full",
    output:
        "{outdir}/ex_{experiment}/{run_id}/reach_metrics.csv",
    group: "post_proc"
    run:
        combined_reach_specific(input[0], input[1], input[2], input[3], output[0])


rule combined_run_reach_metrics:
    input:
         expand("{outdir}/ex_{experiment}/{run_id}/reach_metrics.csv",
                run_id=list(range(num_replicates)), allow_missing=True)
    output:
         "{outdir}/ex_{experiment}/overall_reach_metrics.nc"
    group: "post_proc"
    run:
        ds = combine_replicate_csvs(input, wildcards.experiment, seg_id_idx=True)
        ds.to_netcdf(output[0])


def combine_exper_metrics(experiment_metric_files, outfile, reach_metrics=False):
    ds_list = []
    if reach_metrics:
        reach = '_reach'
    else:
        reach = ''

    for exp in experiment_metric_files:
        ds_list.append(xr.open_dataset(exp))
    ds_comb = xr.concat(ds_list, dim='exp_id')
    ds_comb.to_netcdf(outfile)
    return ds_comb

rule combine_exp_metrics:
    input:
        expand("{outdir}/ex_{experiment}/overall_metrics.nc", experiment=list(config['experiments'].keys()), allow_missing=True)
    output:
        "{outdir}/exp_combined_metrics.nc"
    run:
        combine_exper_metrics(input, output[0])

rule combine_exp_reach_metrics:
    input:
        expand("{outdir}/ex_{experiment}/overall_reach_metrics.nc", experiment=list(config['experiments'].keys()), allow_missing=True)
    output:
        "{outdir}/exp_combined_reach_metrics.nc"
    run:
        combine_exper_metrics(input, output[0])

