import os
import pandas as pd

# add scripts dir to path
code_dir = "/home/jsadler/drb-dl-model/river_dl"
shell.prefix("module load analytics cuda100/toolkit/10.0.130 \n \
              run_training -e /home/jsadler/.conda/envs/rgcn --no-node-list ")

from river_dl.preproc_utils import prep_data, reduce_training_data
from river_dl.postproc_utils import predict_from_file, combine_overall, combined_reach_specific, combine_csvs
data_dir = "/home/jsadler/drb-dl-model/data/in"
out_dir = f"/caldera/projects/usgs/water/iidd/datasci/drb_ml_model/experiments/subset/{config['exp_name']}"
x_vars = ['seg_rain', 'seg_tave_air', 'seginc_swrad', 'seg_length', 'seginc_potet', 'seg_slope', 'seg_humid', 'seg_elev']
num_replicates = 10
train_start_date = '1980-10-01' 
train_end_date = '2004-09-30' 

include: "prep_input.smk"

rule all:
    input:
        f"{out_dir}/exp_combined_metrics.csv",
        f"{out_dir}/exp_combined_reach_metrics.csv"


rule prep_io_data:
    input:
        "{outdir}/temp_red_{temp_red}/{run_id}/reduced_temp",
        "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/reduced_flow",
        f"{data_dir}/uncal_sntemp_input_output_subset",
        f"{data_dir}/distance_matrix_subset.npz",
        catch_attr = f"{data_dir}/seg_attr_drb.feather"
    output:
        "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/prepped.npz"
    params:
        test_start_date='2004-09-30',
        n_test_yr=12
    run:
        prep_data(input[0], input[1], input[2], input[3], x_vars,
                  catch_prop_file=input['catch_attr'],
                  exclude_file=config.get('exclude_file'),
                  test_start_date=params.test_start_date,
                  log_q=True,
                  n_test_yr=params.n_test_yr, out_file=output[0])


# this must be a shell command so that the correct GPU libraries are loaded and used
rule train:
    input:
         "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/prepped.npz"
    output:
        directory("{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/trained_weights/"),
        directory("{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/pretrained_weights/")
    params:
        # getting the base path to put the training outputs in
        # I omit the last slash (hence '[:-1]' so the split works properly
        run_dir=lambda wildcards, output: os.path.split(output[0][:-1])[0],
        rmse_tmp_wgt= 0.5,
        flow_in_temp="-q", 
        pt_epochs=200,
        ft_epochs=100,
    shell:
         """
         "python {code_dir}/train_model.py -o {params.run_dir} -i {input[0]} -m {output[0]} -p {params.pt_epochs} -f {params.ft_epochs} {params.flow_in_temp} --pt_temp_wgt {params.rmse_tmp_wgt} --ft_temp_wgt {params.rmse_tmp_wgt}" 
         """

rule make_predictions:
    input: 
        "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/trained_weights/",
        "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/prepped.npz",
    params:
        hidden_size=20,
        half_tst=True,
    output:
        "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/{partition}_preds.feather",
    group: "post_proc"
    run:
        predict_from_file(input[0], input[1], params.hidden_size,
                          wildcards.partition, output[0], flow_in_temp=True,
                          half_tst=params.half_tst, logged_q=True)


rule combined_metrics:
    input:
         "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/trn_preds.feather",
         "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/tst_preds.feather",
         f"{data_dir}/obs_temp_full",
         f"{data_dir}/obs_flow_full"
    output:
         "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/overall_metrics.csv"
    group: "post_proc"
    run:
        combine_overall(input[0], input[1], input[2], input[3], output[0])


def combine_exp_metrics(csvs):
    df_list = []
    for metric_file in csvs:
        file_parts = metric_file.split('/')
        df = pd.read_csv(metric_file)
        df['run_id'] = int(file_parts[-3])
        df['flow_red'] = float(file_parts[-2].split('_')[-1])
        df['temp_red'] = float(file_parts[-4].split('_')[-1])
        df_list.append(df)
    df = pd.concat(df_list, axis=0)
    return df


rule calc_reach_specific_metrics:
    input: 
         "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/trn_preds.feather",
         "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/tst_preds.feather",
         f"{data_dir}/obs_temp_full",
         f"{data_dir}/obs_flow_full",
    output:
        "{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/reach_metrics.csv",
    group: "post_proc"
    run:
        combined_reach_specific(input[0], input[1], input[2], input[3], output[0])


rule combine_overall_metrics:
    input:
        expand("{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/overall_metrics.csv",
                flow_red=config['flow_reductions'], temp_red=config['temp_reductions'],
                run_id=list(range(num_replicates)), allow_missing=True)
    output:
        "{outdir}/exp_combined_metrics.csv"
    run:
        combined = combine_exp_metrics(input)
        combined.to_csv(output[0], index=False)

rule combine_exp_reach_metrics:
    input:
        expand("{outdir}/temp_red_{temp_red}/{run_id}/flow_red_{flow_red}/reach_metrics.csv",
                flow_red=config['flow_reductions'], temp_red=config['temp_reductions'],
                run_id=list(range(num_replicates)), allow_missing=True)
    output:
        "{outdir}/exp_combined_reach_metrics.csv"
    run:
        combined = combine_exp_metrics(input)
        combined.to_csv(output[0], index=False)

